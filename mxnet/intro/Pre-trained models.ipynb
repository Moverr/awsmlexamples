{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying images with pre-trained Apache MXNet models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download three image classification models from the Apache MXNet [model zoo](http://mxnet.io/model_zoo/).\n",
    "* **VGG-16**: the 2014 classification winner at the [ImageNet Large Scale Visual Recognition Challenge](http://image-net.org/challenges/LSVRC).\n",
    "* **Inception v3**, an evolution of GoogleNet, the 2014 winner for object detection.\n",
    "* **ResNet-152**, the 2015 winner in multiple categories.\n",
    "\n",
    "Why would we want to try multiple models? Why don't we simply pick the one with the best accuracy? As we will see later on, even though these models have been trained on the same data set and optimized for maximum accuracy, they do behave slightly differently on **specific images**: maybe one of the models them will actually do a better job at solving your business problem. **Prediction speed** can vary a lot as well and that's an important factor for many applications.\n",
    "\n",
    "For each model, we need to download two files:\n",
    "* the **symbol** file containing the JSON definition of the neural network: layers, connections, activation functions, etc.\n",
    "* the **weights** file storing values for all neuron weights, a.k.a. parameters, learned by the network during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-symbol.json -O vgg16-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/vgg/vgg16-0000.params -O vgg16-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-symbol.json -O Inception-BN-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/inception-bn/Inception-BN-0126.params -O Inception-BN-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-symbol.json -O resnet-152-symbol.json\n",
    "!wget http://data.dmlc.ml/models/imagenet/resnet/152-layers/resnet-152-0000.params -O resnet-152-0000.params\n",
    "!wget http://data.dmlc.ml/models/imagenet/synset.txt -O synset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first lines of VGG-16 symbol file. We can see the definition of the input layer ('data'), the input weights and the biases for the first convolution layer. A convolution operation is defined ('conv1_1') as well as a Rectified Linear Unit activation function ('relu1_1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -48 vgg16-symbol.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models have been pre-trained on the ImageNet data set which includes over 1.2 million pictures of objects and animals sorted in 1,000 categories. We can view these categories in the synset.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 synset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import cv2,sys,time\n",
    "from collections import namedtuple\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "print \"MXNet version: %s\"  % mx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's load a model.\n",
    "\n",
    "First, we have to load the **weights** and **model description** from file. MXNet calls this a **checkpoint**: indeed, it's good practice to save weights after each training epoch. Once training is complete, we can look at the training log and pick the weights for the best epoch, i.e. the one with the highest validation accuracy: it's quite likely it won't be the very last one!\n",
    "\n",
    "Once loading is complete, we get a *Symbol* object and the weights, a.k.a model parameters. We then create a new *Module* and assign it the input *Symbol*. We could select the *context* where we want to run the model: the default behavior is to use a CPU context. There are two reasons for this:\n",
    "* first, this will allow you to test the notebook even if your machine is not equipped with a GPU :)\n",
    "* second, we're going to predict a single image and we don't have any specific performance requirements. For production applications where you'd want to predict large batches of images with the best possible throughput, a GPU would definitely be the way to go.\n",
    "\n",
    "Then, we bind the input *Symbol* to input data: we have to call it ‘data’ because that’s its name in the **input layer** of the network (remember the first few lines of the JSON file).\n",
    "\n",
    "Finally, we define the **shape** of ‘data’ as 1 x 3 x 224 x 224. ‘224 x 224’ is the image resolution, that’s how the model was trained. ‘3’ is the number of channels : red, green and blue (in this order). ‘1’ is the batch size: we’ll predict one image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadModel(modelname, gpu=False):\n",
    "        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, 0)\n",
    "        arg_params['prob_label'] = mx.nd.array([0])\n",
    "        arg_params['softmax_label'] = mx.nd.array([0])\n",
    "        if gpu:\n",
    "            mod = mx.mod.Module(symbol=sym, context=mx.gpu(0))\n",
    "        else:\n",
    "            mod = mx.mod.Module(symbol=sym)\n",
    "        mod.bind(for_training=False, data_shapes=[('data', (1,3,224,224))])\n",
    "        mod.set_params(arg_params, aux_params)\n",
    "        return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the 1,000 categories stored in the synset.txt file. We'll need the actual descriptions at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCategories():\n",
    "        synsetfile = open('synset.txt', 'r')\n",
    "        synsets = []\n",
    "        for l in synsetfile:\n",
    "                synsets.append(l.rstrip())\n",
    "        return synsets\n",
    "    \n",
    "synsets = loadCategories()\n",
    "print synsets[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function to load an image from file. Remember that the model expects a 4-dimension *NDArray* holding the red, green and blue channels of a single 224 x 224 image. We’re going to use the **OpenCV** library to build this *NDArray* from our input image.\n",
    "\n",
    "Here are the steps:\n",
    "* read the image: this will return a **numpy array** shaped as (image height, image width, 3), with the three channels in **BGR** order (blue, green and red).\n",
    "* convert the image to **RGB**.\n",
    "* resize the image to **224 x 224**.\n",
    "* **reshape** the array from (image height, image width, 3) to (3, image height, image width).\n",
    "* add a **fourth dimension** and build the *NDArray*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareNDArray(filename):\n",
    "        img = cv2.imread(filename)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224,))\n",
    "        img = np.swapaxes(img, 0, 2)\n",
    "        img = np.swapaxes(img, 1, 2)\n",
    "        img = img[np.newaxis, :]\n",
    "        array = mx.nd.array(img)\n",
    "        print array.shape\n",
    "        return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take care of prediction. Our parameters are an image, a model, a list of categories and the number of top categories we'd like to return. \n",
    "\n",
    "Remember that a *Module* object must feed data to a model in **batches**: the common way to do this is to use a **data iterator**. Here, we’d like to predict a single image, so although we could use a data iterator, it’d probably be overkill. Instead, let's create a named tuple, called *Batch*, which will act as a fake iterator by returning our input *NDArray* when its 'data' attribute is referenced.\n",
    "\n",
    "Once the image has been forwarded, the model outputs an *NDArray* holding **1,000 probabilities**, corresponding to the 1,000 categories it has been trained on: the *NDArray* has only one line since batch size is equal to 1. \n",
    "\n",
    "Let’s turn this into an array with *squeeze()*. Then, using *argsort()*, we create a second array holding the **index** of these probabilities sorted in **descending order**. Finally, we return the top n categories and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(filename, model, categories, n):\n",
    "        array = prepareNDArray(filename)\n",
    "        Batch = namedtuple('Batch', ['data'])\n",
    "        t1 = time.time()\n",
    "        model.forward(Batch([array]))\n",
    "        prob = model.get_outputs()[0].asnumpy()\n",
    "        t2 = time.time()\n",
    "        print(\"Predicted in %.2f microseconds\" % (t2-t1))\n",
    "        prob = np.squeeze(prob)\n",
    "        sortedprobindex = np.argsort(prob)[::-1]\n",
    "        \n",
    "        topn = []\n",
    "        for i in sortedprobindex[0:n]:\n",
    "                topn.append((prob[i], categories[i]))\n",
    "        return topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to put everything together. Let's load all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(modelname, gpu=False):\n",
    "        model = loadModel(modelname,gpu)\n",
    "        categories = loadCategories()\n",
    "        return model, categories\n",
    "\n",
    "vgg16,categories = init(\"vgg16\")\n",
    "resnet152,categories = init(\"resnet-152\")\n",
    "inceptionv3,categories = init(\"Inception-BN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classifying images, let's take a closer look to some of the VGG-16 **parameters** we just loaded from the '.params' file. First, let's print the names of all **layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = vgg16.get_params()\n",
    "\n",
    "layers = []\n",
    "for layer in params[0].keys():\n",
    "    layers.append(layer)\n",
    "    \n",
    "layers.sort()    \n",
    "print layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each layer, we see two components: the weights and the biases. Count the weights and you'll see that there are **sixteen** layers: thirteen convolutional layers and three fully connected layers. Now you know why this model is called **VGG-16** :)\n",
    "\n",
    "Now let's print the weights for the last fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print params[0]['fc8_weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the **shape** of this matrix? **1000x4096**. This layer contains **1,000 neurons**: each of them will store the **probability** of the image belonging to a specific category. Each neuron is also fully connected to all **4,096 neurons** in the previous layer ('fc7')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, enough exploring! Now let's use these models to classify our own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"violin.jpg\"\n",
    "\n",
    "display(Image(filename=image))\n",
    "\n",
    "topn = 5\n",
    "print (\"*** VGG16\")\n",
    "print predict(image,vgg16,categories,topn)\n",
    "print (\"*** ResNet-152\")\n",
    "print predict(image,resnet152,categories,topn)\n",
    "print (\"*** Inception v3\")\n",
    "print predict(image,inceptionv3,categories,topn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try again with a **GPU context** this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16,categories = init(\"vgg16\", gpu=True)\n",
    "resnet152,categories = init(\"resnet-152\", gpu=True)\n",
    "inceptionv3,categories = init(\"Inception-BN\", gpu=True)\n",
    "\n",
    "print (\"*** VGG16\")\n",
    "print predict(image,vgg16,categories,topn)\n",
    "print (\"*** ResNet-152\")\n",
    "print predict(image,resnet152,categories,topn)\n",
    "print (\"*** Inception v3\")\n",
    "print predict(image,inceptionv3,categories,topn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If you get an error about GPU support, either your machine or instance is not equipped with a GPU or you're using a version of MXNet that hasn't been built with GPU support (USE_CUDA=1)***\n",
    "\n",
    "The difference in performance is quite noticeable: between **15x** and **20x**. If we predicted **multiple images** at the same time, the gap would widen even more due to the **massive parallelism** of GPU architectures.\n",
    "\n",
    "Now it's time to try your **own images**. Just copy them in the same folder as this notebook, update the filename in the cell above and run the predict() calls again.\n",
    "\n",
    "Have fun with pre-trained models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
